{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Twitter Replies to Corporate Accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script serves as a way to collect tweets directed at corporate Twitter accounts. There is no way to directly search for responses promoted tweets via the Twitter API, but we can find replies to corporate accounts known to publish promoted tweets, then check if the original tweet from these corporate accounts which the reply was directed at was published via \"Twitter Ads.\"\n",
    "\n",
    "To begin, we'll import required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use OAuth to authenticate this app in order to use the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# == OAuth Authentication ==\n",
    "#\n",
    "# This mode of authentication is the new preferred way\n",
    "# of authenticating with Twitter.\n",
    "\n",
    "# The consumer keys can be found on your application's Details\n",
    "# page located at https://dev.twitter.com/apps (under \"OAuth settings\")\n",
    "consumer_key=\"\"\n",
    "consumer_secret=\"\"\n",
    "\n",
    "# The access tokens can be found on your applications's Details\n",
    "# page located at https://dev.twitter.com/apps (located\n",
    "# under \"Your access token\")\n",
    "access_token=\"\"\n",
    "access_token_secret=\"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to search for replies to a large number of corporate accounts, it is most efficient to bundle corporate accounts into sets any query the Twitter API for tweets from any of these accounts. Below, I've bundled the fifty accounts I considered into five groups of ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corporate_names_1 =   ['Dodge',\n",
    "                    'jackdaniels_us',\n",
    "                    'jolly_rancher',\n",
    "                    'syfy',\n",
    "                    'wholefoods',\n",
    "                    'HeinekenSoccer',\n",
    "                    'Freeletics',\n",
    "                    'benandjerrys',\n",
    "                    'DiGiornoPizza',\n",
    "                    'NinjaTrader']\n",
    "\n",
    "corporate_names_2 = ['CoveredCA',\n",
    "                    \"Nautica\",\n",
    "                    \"MountainDew\",\n",
    "                    \"hulu\",\n",
    "                    \"LiveNation\",\n",
    "                    \"AmMadeMovie\",\n",
    "                    \"adidasrunning\",\n",
    "                    \"GenesisUSA\",\n",
    "                    \"AmericanGodsSTZ\",\n",
    "                    \"gsuite\"]\n",
    "\n",
    "corporate_names_3 = ['SamsungBizUSA',\n",
    "                    \"OnStar\",\n",
    "                    \"NyQuilDayQuil\",\n",
    "                    \"HARDER\",\n",
    "                    \"JulianBakery\",\n",
    "                    \"Total\",\n",
    "                    \"DennysDiner\",\n",
    "                    \"AtlanticNet\",\n",
    "                    \"NortonOnline\",\n",
    "                    \"AmericanExpress\"]\n",
    "\n",
    "corporate_names_4 = ['HP',\n",
    "                    \"sprint\",\n",
    "                    \"pandoramusic\",\n",
    "                    \"constitutioncenter\",\n",
    "                    \"IBMsecurity\",\n",
    "                    \"pizzahut\",\n",
    "                    \"mesosphere\",\n",
    "                    \"transformers\",\n",
    "                    \"HPE\",\n",
    "                    \"NBA\"]\n",
    "\n",
    "corporate_names_5 = ['McAfee',\n",
    "                    \"beatsbydre\",\n",
    "                    \"chipotletweets\",\n",
    "                    \"HUGOBOSS\",\n",
    "                    \"MakersMark\",\n",
    "                    \"DairyQueen\",\n",
    "                    \"PruTalent\",\n",
    "                    \"TwitterMktg\",\n",
    "                    \"Prudential\",\n",
    "                    \"xbox\"]\n",
    "\n",
    "corporate_accounts = [corporate_names_1,\n",
    "                     corporate_names_2,\n",
    "                     corporate_names_3,\n",
    "                     corporate_names_4,\n",
    "                     corporate_names_5]\n",
    "\n",
    "number_of_groups = len(corporate_accounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define two useful functions. The first will generate the string required by the Twitter API to search all replies to corporate accounts within a given group. The second will return the ID of the oldest tweet within a set of tweet objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_search_string(names):\n",
    "    return ' '.join([\"to:%s OR\" % i if i != names[-1] else \"to:%s\" % i for i in names])\n",
    "\n",
    "def get_old_min_id(l):\n",
    "    return min([tweet.id for tweet in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create the search strings for each group of corporate accounts defined above. We then collect all tweets that respond to these accounts. Note that the script below is designed to analyze the contents of the current directory to determine the earliest tweets that have already been captured by the script. The script will then only search for earlier tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets_at_corporate_accounts_13.pickle\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-75f4c5c3b83e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mold_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmax_id_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_old_min_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmax_id_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_old_min_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mmax_id_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_old_min_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group_3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mmax_id_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_old_min_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group_4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-75f4c5c3b83e>\u001b[0m in \u001b[0;36mget_old_min_id\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_old_min_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msearch_string_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_search_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorporate_names_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "search_strings = []\n",
    "\n",
    "for group in corporate_accounts:\n",
    "    search_strings.append(make_search_string(group))\n",
    "\n",
    "tweets_at_corporate_accounts = {}\n",
    "\n",
    "for i in range(number_of_groups):\n",
    "    tweets_at_corporate_accounts['group_%d' % i] = []\n",
    "\n",
    "file_re = re.compile(r'tweets_at_corporate_accounts_\\d\\d?\\.pickle')\n",
    "\n",
    "final_data_file = ''\n",
    "\n",
    "max_ids = []\n",
    "\n",
    "for filename in [file_re.findall(i)[0] for i in os.listdir('.') if file_re.findall(i) != []]:\n",
    "    file_count = int(filename.strip(\"tweets_at_corporate_accounts_\\.pickle\"))\n",
    "    if final_data_file:\n",
    "        if file_count > int(final_data_file.strip(\"tweets_at_corporate_accounts_\\.pickle\")) and os.path.getsize('./%s' % filename) != 0:\n",
    "            final_data_file = filename\n",
    "    elif os.path.getsize('./%s' % filename) != 0:\n",
    "        final_data_file = filename\n",
    "        \n",
    "print final_data_file\n",
    "changed_groups = False\n",
    "        \n",
    "if final_data_file:\n",
    "    count = int(final_data_file.strip(\"tweets_at_corporate_accounts_\\.pickle\")) + 1\n",
    "else:\n",
    "    count = 1\n",
    "if not changed_groups:\n",
    "    f = open(final_data_file,'rb')\n",
    "    old_tweets = pickle.load(f)\n",
    "    \n",
    "    for i in range(number_of_groups):\n",
    "        max_ids.append(get_old_min_id(old_tweets['group_%d' % i])\n",
    "else:\n",
    "    for i in range(number_of_groups):\n",
    "        max_ids.append(10e20)\n",
    "    \n",
    "tweets_file = open('tweets_at_corporate_accounts_%s.pickle' % str(count),'w')\n",
    "\n",
    "while True:\n",
    "    start = time.time()\n",
    "    try:\n",
    "        new_ids = []\n",
    "        for i in range(number_of_groups):\n",
    "            new_batch = api.search(q=search_strings[i] + \" -filter:retweets\", count = 100, max_id = max_ids[i], lang=\"en\")\n",
    "            tweets_at_corporate_accounts['group_%d' % i] += [t for t in new_batch if t.in_reply_to_status_id]\n",
    "            max_ids[i] = min([t.id for t in new_batch])\n",
    "        \n",
    "        print sum([len(tweets_at_corporate_accounts[i]) for i in tweets_at_corporate_accounts])\n",
    "    except tweepy.RateLimitError:\n",
    "        if len(tweets_at_corporate_accounts) > 100000:\n",
    "            pickle.dump(tweets_at_corporate_accounts,tweets_file)\n",
    "            tweets_file.close()\n",
    "            break\n",
    "        else:\n",
    "            print \"rate limit\"\n",
    "            pickle.dump(tweets_at_corporate_accounts,tweets_file)\n",
    "            tweets_file.close()\n",
    "            for i in range(number_of_groups):\n",
    "                tweets_at_corporate_accounts['group_%d' % i] = []\n",
    "            count += 1\n",
    "            tweets_file = open('tweets_at_corporate_accounts_%s.pickle' % str(count),'w')\n",
    "            end = time.time()\n",
    "            print (900 - (end-start)) / 60\n",
    "            time.sleep(900 - (end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
